{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2938025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweetnlp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6abfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 1099: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[38;5;28mprint\u001b[39m(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mUser\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mSentiment\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mtweetnlp\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mquestion_5.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x93 in position 1099: invalid start byte"
     ]
    }
   ],
   "source": [
    "df = print(pd.read_csv(\"C:\\\\Users\\\\User\\\\Sentiment\\\\tweetnlp\\\\question_8.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de856c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust CSV reading function\n",
    "def read_csv_with_encoding_detection(file_path):\n",
    "    \"\"\"\n",
    "    Attempts to read a CSV file by trying different encodings.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame or None: The loaded DataFrame or None if failed\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        # List available CSV files in the directory\n",
    "        directory = os.path.dirname(file_path)\n",
    "        if os.path.exists(directory):\n",
    "            csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "            if csv_files:\n",
    "                print(\"Available CSV files in the directory:\")\n",
    "                for file in csv_files:\n",
    "                    print(f\"  - {file}\")\n",
    "        return None\n",
    "    \n",
    "    # Common encodings to try\n",
    "    encodings = [\n",
    "        'utf-8',           # Most common modern encoding\n",
    "        'latin-1',         # Common for Western European languages\n",
    "        'iso-8859-1',      # Similar to latin-1\n",
    "        'cp1252',          # Windows encoding\n",
    "        'utf-16',          # Unicode 16-bit\n",
    "        'utf-8-sig',       # UTF-8 with BOM\n",
    "        'cp850',           # DOS encoding\n",
    "        'ascii'            # Basic ASCII\n",
    "    ]\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"üîç Trying encoding: {encoding}\")\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"‚úÖ Success! File loaded with encoding: {encoding}\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            return df\n",
    "            \n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"‚ùå UnicodeDecodeError with {encoding}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Other error with {encoding}: {str(e)[:60]}...\")\n",
    "            continue\n",
    "    \n",
    "    # If all encodings failed, try with error handling\n",
    "    print(\"\\nüîß Trying with error handling (replacing problematic characters)...\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', errors='replace')\n",
    "        print(\"‚úÖ Loaded with character replacement (some characters may appear as ÔøΩ)\")\n",
    "        return df\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin-1', errors='ignore')\n",
    "            print(\"‚úÖ Loaded with character ignoring (some characters may be missing)\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Final attempt failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Now try to load your CSV file\n",
    "print(\"Loading CSV file with encoding detection...\")\n",
    "print(\"=\"*50)\n",
    "df = read_csv_with_encoding_detection(r\"C:\\Users\\User\\Sentiment\\tweetnlp\\question1.csv\")\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FILE LOADED SUCCESSFULLY!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\n‚ùå Unable to load the file. Please check the file path and format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92a15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (12, 1)\n",
      "\n",
      "Column names: ['Q9: Do Brands openly share their policies and guidelines regarding the use of AI?']\n",
      "\n",
      "First few rows:\n",
      "  Q9: Do Brands openly share their policies and guidelines regarding the use of AI?\n",
      "0  Some do, some do not, due to legal grey areas ...                               \n",
      "1  In the current environment, I don't see brands...                               \n",
      "2                     Some brands do but not fully.                                \n",
      "3  No..not at the moment but this will change soo...                               \n",
      "4  Yes but I have not in a way that is easy or di...                               \n"
     ]
    }
   ],
   "source": [
    "# Let's examine the structure of the CSV file\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e5360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweetnlp sentiment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1235: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1001: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the sentiment analysis model\n",
    "print(\"Loading tweetnlp sentiment model...\")\n",
    "model = tweetnlp.load_model(\"sentiment\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baf3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sentiment analysis on all responses...\n",
      "============================================================\n",
      "Response 1: neutral\n",
      "Text: Some do, some do not, due to legal grey areas but i believe that they should...\n",
      "----------------------------------------\n",
      "Response 2: negative\n",
      "Text: In the current environment, I don't see brands share their policies and guidelines regarding the use...\n",
      "----------------------------------------\n",
      "Response 3: neutral\n",
      "Text: Some brands do but not fully. ...\n",
      "----------------------------------------\n",
      "Response 4: neutral\n",
      "Text: No..not at the moment but this will change soon as mandatory disclosure requirements are impending ...\n",
      "----------------------------------------\n",
      "Response 5: neutral\n",
      "Text: Yes but I have not in a way that is easy or digestible. I would assume it is somewhere in the terms ...\n",
      "----------------------------------------\n",
      "Response 6: neutral\n",
      "Text: no...\n",
      "----------------------------------------\n",
      "Response 7: positive\n",
      "Text: Yes and they should ...\n",
      "----------------------------------------\n",
      "Response 8: neutral\n",
      "Text: Not sure, but it would not matter (see answer #7). It is also commonly hidden behind legalese....\n",
      "----------------------------------------\n",
      "Response 9: neutral\n",
      "Text: They have it somewhere if and when called upon to reference it. ...\n",
      "----------------------------------------\n",
      "Response 10: neutral\n",
      "Text: Most of the time, however, it is usually in a long document, which forces customers who are concerne...\n",
      "----------------------------------------\n",
      "Response 4: neutral\n",
      "Text: No..not at the moment but this will change soon as mandatory disclosure requirements are impending ...\n",
      "----------------------------------------\n",
      "Response 5: neutral\n",
      "Text: Yes but I have not in a way that is easy or digestible. I would assume it is somewhere in the terms ...\n",
      "----------------------------------------\n",
      "Response 6: neutral\n",
      "Text: no...\n",
      "----------------------------------------\n",
      "Response 7: positive\n",
      "Text: Yes and they should ...\n",
      "----------------------------------------\n",
      "Response 8: neutral\n",
      "Text: Not sure, but it would not matter (see answer #7). It is also commonly hidden behind legalese....\n",
      "----------------------------------------\n",
      "Response 9: neutral\n",
      "Text: They have it somewhere if and when called upon to reference it. ...\n",
      "----------------------------------------\n",
      "Response 10: neutral\n",
      "Text: Most of the time, however, it is usually in a long document, which forces customers who are concerne...\n",
      "----------------------------------------\n",
      "Response 11: neutral\n",
      "Text: To my knowledge, I do not think so. It could be embedded in their terms and conditions that customer...\n",
      "----------------------------------------\n",
      "Response 12: neutral\n",
      "Text: Big brands often share their policies and guidelines in terms of transparency reports. These reports...\n",
      "----------------------------------------\n",
      "\n",
      "Sentiment analysis completed for 12 responses!\n",
      "Response 11: neutral\n",
      "Text: To my knowledge, I do not think so. It could be embedded in their terms and conditions that customer...\n",
      "----------------------------------------\n",
      "Response 12: neutral\n",
      "Text: Big brands often share their policies and guidelines in terms of transparency reports. These reports...\n",
      "----------------------------------------\n",
      "\n",
      "Sentiment analysis completed for 12 responses!\n"
     ]
    }
   ],
   "source": [
    "# Perform sentiment analysis on all responses\n",
    "column_name = df.columns[0]  # Get the column name\n",
    "responses = df[column_name].tolist()\n",
    "\n",
    "print(\"Performing sentiment analysis on all responses...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sentiments = []\n",
    "for i, response in enumerate(responses):\n",
    "    if pd.notna(response):  # Check if response is not NaN\n",
    "        sentiment = model.predict(str(response))\n",
    "        sentiments.append(sentiment['label'])\n",
    "        print(f\"Response {i+1}: {sentiment['label']}\")\n",
    "        print(f\"Text: {response[:100]}...\")  # Show first 100 characters\n",
    "        print(\"-\" * 40)\n",
    "    else:\n",
    "        sentiments.append('N/A')\n",
    "        print(f\"Response {i+1}: N/A (empty response)\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Add sentiment results to the dataframe\n",
    "df['sentiment'] = sentiments\n",
    "print(f\"\\nSentiment analysis completed for {len(responses)} responses!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a3062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with Sentiment Analysis:\n",
      "==================================================\n",
      "   Q9: Do Brands openly share their policies and guidelines regarding the use of AI?  \\\n",
      "0   Some do, some do not, due to legal grey areas ...                                  \n",
      "1   In the current environment, I don't see brands...                                  \n",
      "2                      Some brands do but not fully.                                   \n",
      "3   No..not at the moment but this will change soo...                                  \n",
      "4   Yes but I have not in a way that is easy or di...                                  \n",
      "5                                                  no                                  \n",
      "6                                Yes and they should                                   \n",
      "7   Not sure, but it would not matter (see answer ...                                  \n",
      "8   They have it somewhere if and when called upon...                                  \n",
      "9   Most of the time, however, it is usually in a ...                                  \n",
      "10  To my knowledge, I do not think so. It could b...                                  \n",
      "11  Big brands often share their policies and guid...                                  \n",
      "\n",
      "   sentiment  \n",
      "0    neutral  \n",
      "1   negative  \n",
      "2    neutral  \n",
      "3    neutral  \n",
      "4    neutral  \n",
      "5    neutral  \n",
      "6   positive  \n",
      "7    neutral  \n",
      "8    neutral  \n",
      "9    neutral  \n",
      "10   neutral  \n",
      "11   neutral  \n",
      "\n",
      "==================================================\n",
      "SENTIMENT ANALYSIS SUMMARY\n",
      "==================================================\n",
      "Sentiment Distribution:\n",
      "  Neutral: 10 responses (83.3%)\n",
      "  Negative: 1 responses (8.3%)\n",
      "  Positive: 1 responses (8.3%)\n",
      "\n",
      "Total responses analyzed: 12\n",
      "\n",
      "==================================================\n",
      "KEY INSIGHTS\n",
      "==================================================\n",
      "‚Ä¢ Most common sentiment: Neutral (10 responses)\n",
      "‚Ä¢ Response diversity: 3 different sentiment categories detected\n"
     ]
    }
   ],
   "source": [
    "# Display the updated dataframe with sentiment analysis\n",
    "print(\"Updated DataFrame with Sentiment Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(\"Sentiment Distribution:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {sentiment.capitalize()}: {count} responses ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal responses analyzed: {len(df)}\")\n",
    "\n",
    "# Show some insights\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "most_common = sentiment_counts.index[0]\n",
    "print(f\"‚Ä¢ Most common sentiment: {most_common.capitalize()} ({sentiment_counts.iloc[0]} responses)\")\n",
    "print(f\"‚Ä¢ Response diversity: {len(sentiment_counts)} different sentiment categories detected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
